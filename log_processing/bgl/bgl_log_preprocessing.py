"""
BGL Log Preprocessing for BERT Anomaly Detection
Optimized for semantic preservation and BERT efficiency
"""

import re
from typing import List, Set
from pathlib import Path


class BGLLogPreprocessor:
    """
    Preprocessor untuk BGL (BlueGene/L) log format
    
    Format input:
    [UNIX_TS] [DATE] [NODE] [TIMESTAMP] [NODE] [COMPONENT] [SUBSYSTEM] [LEVEL] [MESSAGE]
    
    Format output:
    [component] [subsystem] [level] [normalized_message]
    """
    
    def __init__(self):
        # Regex patterns for normalization
        self.patterns = {
            # Numbers (integers, floats, scientific notation)
            'number': re.compile(r'\b\d+\.?\d*([eE][+-]?\d+)?\b'),
            
            # Hexadecimal values
            'hex': re.compile(r'\b0[xX][0-9a-fA-F]+\b'),
            
            # Node IDs (BGL format: RXX-MX-NX-C:JXX-UXX or similar)
            'node': re.compile(r'\b[A-Z]\d{2,3}-[A-Z]\d+-[A-Z]{1,2}(-[A-Z])?:[A-Z]\d{2}-[A-Z]\d{2}\b'),
            
            # IP addresses (IPv4)
            'ip': re.compile(r'\b(?:\d{1,3}\.){3}\d{1,3}(?::\d+)?\b'),
            
            # File paths (Unix/Linux style)
            'path': re.compile(r'(?:/[a-zA-Z0-9_\-\.]+)+/?'),
            
            # URLs
            'url': re.compile(r'https?://[^\s]+'),
            
            # Email addresses
            'email': re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'),
            
            # Timestamps (various formats)
            'timestamp': re.compile(r'\d{4}[-/]\d{2}[-/]\d{2}[-\s]\d{2}[:.]\d{2}[:.]\d{2}(?:\.\d+)?'),
        }
        
        # Track statistics
        self.stats = {
            'total_lines': 0,
            'processed_lines': 0,
            'skipped_lines': 0,
            'empty_lines': 0,
            'malformed_lines': 0,
        }
    
    def parse_bgl_line(self, line: str) -> dict:
        """
        Parse BGL log line into components
        
        Supports TWO formats:
        
        Format 1 (with - prefix):
        - [0] UNIX timestamp [1] Date [2] Node [3] Timestamp [4] Node [5] Component [6] Subsystem [7] Level [8] Message
        
        Format 2 (with label prefix):
        [0] Label [1] UNIX timestamp [2] Date [3] Node [4] Timestamp [5] Node [6] Component [7] Subsystem [8] Level [9] Message
        
        Note: Some logs have "- " prefix, need to skip it
        """
        # Remove leading "- " if present
        line = line.strip()
        if line.startswith('- '):
            line = line[2:]
        
        # Check if first field is a label (not a timestamp)
        first_word = line.split(maxsplit=1)[0]
        
        if first_word.isalpha() or (not first_word.isdigit()):
            # Format 2: has label prefix (e.g., "KERNMNTF")
            # [Label] [UNIX_TS] [Date] [Node] [Timestamp] [Node] [Component] [Subsystem] [Level] [Message...]
            parts = line.split(maxsplit=9)
            if len(parts) < 10:
                return None
            return {
                'component': parts[6],
                'subsystem': parts[7],
                'level': parts[8],
                'message': parts[9]
            }
        else:
            # Format 1: no label prefix
            # [UNIX_TS] [Date] [Node] [Timestamp] [Node] [Component] [Subsystem] [Level] [Message...]
            parts = line.split(maxsplit=8)
            if len(parts) < 9:
                return None
            return {
                'component': parts[5],
                'subsystem': parts[6],
                'level': parts[7],
                'message': parts[8]
            }
    
    def normalize_message(self, message: str) -> str:
        """
        Normalize message by replacing variables with tokens
        """
        # Apply normalization in specific order (important!)
        
        # 1. URLs first (before other patterns)
        message = self.patterns['url'].sub('<URL>', message)
        
        # 2. Email addresses
        message = self.patterns['email'].sub('<EMAIL>', message)
        
        # 3. IP addresses (before numbers to avoid partial matches)
        message = self.patterns['ip'].sub('<IP>', message)
        
        # 4. File paths
        message = self.patterns['path'].sub('<PATH>', message)
        
        # 5. Timestamps
        message = self.patterns['timestamp'].sub('', message)
        
        # 6. Node IDs
        message = self.patterns['node'].sub('<NODE>', message)
        
        # 7. Hexadecimal values (before general numbers)
        message = self.patterns['hex'].sub('<HEX>', message)
        
        # 8. Numbers (integers and floats)
        message = self.patterns['number'].sub('<NUM>', message)
        
        # 9. Lowercase
        message = message.lower()
        
        # 10. Remove special characters (keep only alphanumeric, spaces, <>)
        message = re.sub(r'[^\w\s<>]', ' ', message)
        
        # 11. Collapse multiple spaces
        message = re.sub(r'\s+', ' ', message)
        
        # 12. Strip whitespace
        message = message.strip()
        
        return message
    
    def preprocess_line(self, line: str) -> str:
        """
        Preprocess a single BGL log line
        
        Returns:
            Preprocessed string: "[component] [subsystem] [level] [message]"
            or empty string if line should be skipped
        """
        self.stats['total_lines'] += 1
        
        # Skip empty lines
        if not line or not line.strip():
            self.stats['empty_lines'] += 1
            return ""
        
        # Parse line
        parsed = self.parse_bgl_line(line)
        if not parsed:
            self.stats['malformed_lines'] += 1
            return ""
        
        # Normalize message
        normalized_msg = self.normalize_message(parsed['message'])
        
        # Skip if message becomes empty after normalization
        if not normalized_msg:
            self.stats['skipped_lines'] += 1
            return ""
        
        # Combine fields WITHOUT node ID
        # Format: [component] [subsystem] [level] [message]
        preprocessed = f"{parsed['component'].lower()} {parsed['subsystem'].lower()} {parsed['level'].lower()} {normalized_msg}"
        
        self.stats['processed_lines'] += 1
        return preprocessed
    
    def preprocess_logs(self, lines: List[str]) -> List[str]:
        """
        Preprocess multiple log lines
        
        Returns:
            List of preprocessed log lines
        """
        preprocessed_lines = []
        
        for line in lines:
            preprocessed = self.preprocess_line(line)
            if preprocessed:  # Only add non-empty lines
                preprocessed_lines.append(preprocessed)
        
        return preprocessed_lines
    
    def remove_duplicates(self, logs: List[str]) -> List[str]:
        """
        Remove duplicate log entries while preserving order
        """
        seen = set()
        unique_logs = []
        
        for log in logs:
            if log not in seen:
                seen.add(log)
                unique_logs.append(log)
        
        return unique_logs
    
    def print_stats(self):
        """Print preprocessing statistics"""
        print("\n" + "="*80)
        print("ðŸ“Š PREPROCESSING STATISTICS")
        print("="*80)
        print(f"\nâœ“ Total lines read: {self.stats['total_lines']:,}")
        print(f"âœ“ Successfully processed: {self.stats['processed_lines']:,}")
        print(f"âœ“ Empty lines: {self.stats['empty_lines']:,}")
        print(f"âœ“ Malformed lines: {self.stats['malformed_lines']:,}")
        print(f"âœ“ Skipped (no content): {self.stats['skipped_lines']:,}")
        
        if self.stats['total_lines'] > 0:
            success_rate = (self.stats['processed_lines'] / self.stats['total_lines']) * 100
            print(f"\nâœ“ Success rate: {success_rate:.2f}%")
    
    def reset_stats(self):
        """Reset statistics counters"""
        for key in self.stats:
            self.stats[key] = 0


def process_bgl_file(input_file: str, output_file: str, remove_duplicates: bool = True):
    """
    Process BGL log file and save preprocessed output
    
    Args:
        input_file: Path to input BGL log file
        output_file: Path to output preprocessed file
        remove_duplicates: Whether to remove duplicate entries
    """
    print("="*80)
    print("ðŸ”§ BGL LOG PREPROCESSING")
    print("="*80)
    print(f"\nâœ“ Input file: {input_file}")
    print(f"âœ“ Output file: {output_file}")
    print(f"âœ“ Remove duplicates: {remove_duplicates}")
    
    # Initialize preprocessor
    preprocessor = BGLLogPreprocessor()
    
    # Read input file
    print(f"\nðŸ“– Reading input file...")
    with open(input_file, 'r', encoding='utf-8', errors='ignore') as f:
        lines = f.readlines()
    
    print(f"âœ“ Read {len(lines):,} lines")
    
    # Preprocess
    print(f"\nâš™ï¸  Preprocessing logs...")
    preprocessed_lines = preprocessor.preprocess_logs(lines)
    
    # Remove duplicates if requested
    original_count = len(preprocessed_lines)
    if remove_duplicates:
        print(f"\nðŸ”„ Removing duplicates...")
        preprocessed_lines = preprocessor.remove_duplicates(preprocessed_lines)
        duplicates_removed = original_count - len(preprocessed_lines)
        print(f"âœ“ Removed {duplicates_removed:,} duplicates ({duplicates_removed/original_count*100:.2f}%)")
    
    # Save output
    print(f"\nðŸ’¾ Saving preprocessed logs...")
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for line in preprocessed_lines:
            f.write(line + '\n')
    
    print(f"âœ“ Saved {len(preprocessed_lines):,} lines to {output_file}")
    
    # Print statistics
    preprocessor.print_stats()
    
    # Size comparison
    input_size = Path(input_file).stat().st_size / (1024 * 1024)
    output_size = output_path.stat().st_size / (1024 * 1024)
    reduction = (1 - output_size / input_size) * 100
    
    print("\n" + "="*80)
    print("ðŸ“¦ FILE SIZE COMPARISON")
    print("="*80)
    print(f"\nâœ“ Input size: {input_size:.2f} MB")
    print(f"âœ“ Output size: {output_size:.2f} MB")
    print(f"âœ“ Reduction: {reduction:.2f}%")
    
    print("\n" + "="*80)
    print("âœ… PREPROCESSING COMPLETE!")
    print("="*80)


if __name__ == "__main__":
    # Example usage
    import sys
    
    # Default paths
    default_input = "../../dataset/BGL/BGL.log"
    default_output = "../../after_preprocessed_dataset/after_preprocessed_bgl.txt"
    
    # Filter out Jupyter/IPython arguments (e.g., '-f')
    filtered_args = [arg for arg in sys.argv if not arg.startswith('-f')]
    
    # Check if custom paths provided
    if len(filtered_args) >= 3:
        input_file = filtered_args[1]
        output_file = filtered_args[2]
    elif len(filtered_args) == 2 and filtered_args[1] in ['--help', '-h']:
        print("Usage: python bgl_log_preprocessing.py [input_file] [output_file] [--keep-duplicates]")
        print("\nDefault:")
        print(f"  Input:  {default_input}")
        print(f"  Output: {default_output}")
        print("\nExample:")
        print("  python bgl_log_preprocessing.py")
        print("  python bgl_log_preprocessing.py BGL.log BGL_preprocessed.txt")
        print("  python bgl_log_preprocessing.py BGL.log BGL_preprocessed.txt --keep-duplicates")
        sys.exit(0)
    else:
        # Use default paths
        input_file = default_input
        output_file = default_output
    
    remove_dups = "--keep-duplicates" not in filtered_args
    
    process_bgl_file(input_file, output_file, remove_duplicates=remove_dups)
